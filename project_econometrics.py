# -*- coding: utf-8 -*-
"""Project Econometrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vlhm3MBWDVKIFJKxcyAFdXF1Xz5PMJM1
"""

from google.colab import files

# Upload the dataset
uploaded = files.upload()

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split

# Load the dataset
data = pd.read_csv('real_estate_data.csv')

# Display the first few rows of the dataset
print("\nFirst five rows of the dataset:")
print(data.head())

# Drop duplicates
duplicates = data.duplicated().sum()
if duplicates > 0:
  data = data.drop_duplicates()
print(f"Dropped {duplicates} duplicate rows.")

"""s"""

# Identify and handle outliers using the IQR method
print("\nOutliers detected and handled:")
for column in data.select_dtypes(include=np.number).columns:
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    before_rows = data.shape[0]
    data = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]
    after_rows = data.shape[0]
    print(f"{column}: Removed {before_rows - after_rows} outliers.")

"""Outliers were detected and handled. For most columns, no outliers were found,

except for the "Price" column, where 1 outlier was removed.
"""

# Check for missing values and visualize them
print("\nMissing values:")
missing_values = data.isnull().sum()
print(missing_values)
plt.figure(figsize=(10, 6))
sns.heatmap(data.isnull(), cbar=False, cmap="viridis")
plt.title("Heatmap of Missing Values", fontsize=14)
plt.show()

# Compute descriptive statistics
print("\nDescriptive statistics after handling missing values:")
print(data.describe())

# Scatter plots for relationships with the target variable (Price)
features = ['Square_Feet', 'Num_Bedrooms', 'Num_Bathrooms', 'Num_Floors', 'Year_Built']
if 'Price' in data.columns:
  for feature in features:
    if feature in data.columns:
      plt.figure(figsize=(8, 5))
      sns.scatterplot(x=data[feature], y=data['Price'])
      plt.title(f'{feature} vs Price', fontsize=14)
      plt.xlabel(feature)
      plt.ylabel('Price')
      plt.show()

"""The graphs shows that Price has a moderate positive correlation with Year Built and Square feet but there is weak relationship with other variables."""

# Check correlations with the target variable
target = 'Price'  # Define your dependent variable
independent_vars = [col for col in data.columns if col != target]

print("\nCorrelations of independent variables with the target variable:")
correlations = data[independent_vars].corrwith(data[target])
print(correlations)

""" "Square_Feet" (0.56) and "Num_Bedrooms" (0.56) have a moderate positive correlation with "Price," while "Has_Garden" (0.10) and "Has_Pool" (0.13) show weak correlations."""

# Visualize correlation of features with the target variable
plt.figure(figsize=(8, 5))
correlations.sort_values().plot(kind='bar', color='skyblue')
plt.title('Correlation of Features with Target (Price)', fontsize=14)
plt.ylabel('Correlation Coefficient')
plt.show()

# Visualize the overall correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(data.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Matrix", fontsize=14)
plt.show()

"""The matrix provided the same discussed results before and no high correlations among the independant variables."""

# Identify and display highly correlated independent variables (threshold > 0.8)
threshold = 0.8
highly_correlated = []

correlation_matrix = data[independent_vars].corr()
for col in correlation_matrix.columns:
    for row in correlation_matrix.index:
        if abs(correlation_matrix.loc[row, col]) > threshold and row != col:
            pair = tuple(sorted([row, col]))
            if pair not in highly_correlated:
                highly_correlated.append(pair)

print("\nHighly correlated pairs of independent variables (correlation > 0.8):")
print(highly_correlated)

"""This code verify that there is no highly correlated pairs of independent variables as shown in the correlation matrix."""

# Split the dataset into train and test sets
X = data[independent_vars]
y = data[target]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"\nTrain-test split completed:")
print(f"Train set size: {X_train.shape[0]} rows")
print(f"Test set size: {X_test.shape[0]} rows")

# Display a preview of the train-test split
print("\nPreview of X_train:")
print(X_train.head())
print("\nPreview of y_train:")
print(y_train.head())
print("\nPreview of X_test:")
print(X_test.head())
print("\nPreview of y_test:")
print(y_test.head())

from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

y_pred = model.predict(X_test)
print(f"MAE: {mean_absolute_error(y_test, y_pred)}")
print(f"MSE: {mean_squared_error(y_test, y_pred)}")
print(f"R²: {r2_score(y_test, y_pred)}")

"""
Interpretation:

MAE: The model's predictions are off by an average of 22,918.90 units.
MSE: The squared error is 827,350,549, indicating some larger prediction errors.
R²: The model explains 94% of the variance in "Price," suggesting a strong fit.

Conclusion:
Given the high R² and reasonable MAE, the linear regression model is a good fit for the data."""

# Save the cleaned data to a CSV file
data.to_csv('cleaned_data.csv', index=False)

# To ensure the file is saved to your current working directory, print the file path
import os
print(f"File saved at: {os.path.abspath('cleaned_data.csv')}")
from google.colab import files

# Download the file
files.download('cleaned_data.csv')

import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as snss
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from statsmodels.stats.outliers_influence import variance_inflation_factor
from itertools import combinations

# Load the cleaned dataset
file_path = "cleaned_data.csv"  # Update with correct path if necessary
df = pd.read_csv(file_path)

# Define independent variables (X) and dependent variable (y)
X = df.drop(columns=["Price"])
y = df["Price"]

# Add a constant for intercept in the model
X = sm.add_constant(X)

# Step 2: Estimate full model and check VIFs
def calculate_vif(X):
    vif_data = pd.DataFrame()
    vif_data["Feature"] = X.columns
    vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif_data

vif_data = calculate_vif(X)
print("Initial VIF values:")
print(vif_data)

# Step 3: Remove variables with VIF > 5 iteratively
removed_vars = []
while vif_data["VIF"].max() > 5:
    remove_var = vif_data.loc[vif_data["VIF"].idxmax(), "Feature"]
    if remove_var == "const":
        break  # Stop if constant has the highest VIF
    X = X.drop(columns=[remove_var])
    removed_vars.append(remove_var)
    vif_data = calculate_vif(X)

print("Removed variables due to high VIF:", removed_vars if removed_vars else "None")
print("Final VIF values:")
print(vif_data)

"""Interpretation:

Initial VIF Values:

All independent variables have VIF values close to 1, which indicates low multicollinearity.
The constant term (const) has a high VIF, but this is expected and does not affect model validity.

VIF Check Results:

Since no independent variable has a VIF greater than 5, no variables were removed due to multicollinearity.
Final VIF Values:

The VIF values remain unchanged, confirming that all predictors in the model are independent and do not exhibit strong correlations.

Conclusion:

Multicollinearity is not a concern in this dataset.
All independent variables will be retained for further model-building steps.
"""

# Step 4: Perform best subsets regression based on Cp
def best_subset_regression(X, y):
    best_models = []
    k = len(X.columns)  # Number of independent variables
    for num_features in range(1, k + 1):
        for subset in combinations(X.columns, num_features):
            X_subset = X[list(subset)]
            model = sm.OLS(y, sm.add_constant(X_subset)).fit()
            cp = (model.ssr / model.mse_resid) - (len(y) - 2 * (num_features + 1))
            best_models.append((subset, cp, model.rsquared_adj, model.bse.mean(), model))
    return sorted(best_models, key=lambda x: x[1])  # Sort by Cp

best_models = best_subset_regression(X, y)

# Step 5: Filter models where Cp <= k + 1
filtered_models = [m for m in best_models if m[1] <= (len(X.columns) + 1)]

print("All subset regression results (sorted by Cp):")
for model in best_models:
    print(f"Variables: {model[0]}, Cp: {model[1]:.2f}, Adjusted R²: {model[2]:.4f}, Std Error: {model[3]:.4f}")

if filtered_models:
    print("\nFiltered models with Cp <= k+1:")
    for model in filtered_models:
        print(f"Variables: {model[0]}, Cp: {model[1]:.2f}, Adjusted R²: {model[2]:.4f}, Std Error: {model[3]:.4f}")
else:
    print("No models met the Cp criteria.")

# Step 6: Select the best model based on highest Adjusted R² and lowest Std Error
if filtered_models:
    best_model = max(filtered_models, key=lambda x: (x[2], -x[3]))
else:
    best_model = best_models[0]  # If no models met the Cp criteria, select the best from all

print("\nSelected Best Model:")
print(f"Variables: {best_model[0]}, Cp: {best_model[1]:.2f}, Adjusted R²: {best_model[2]:.4f}, Std Error: {best_model[3]:.4f}")
print(best_model[4].summary())

"""Step 4: Best Subsets Regression Results

The function evaluates all possible combinations of independent variables.
Each model's Cp, Adjusted R², and Standard Error are computed.
The results are sorted by Cp, allowing for easy identification of the most efficient models.

Step 5: Filtering Models Based on Cp ≤ (k+1)
Models where Cp > k+1 (where k is the number of predictors) are removed.
The remaining models are displayed, ensuring only statistically efficient models move forward.

Step 6: Selecting the Best Model
Among the filtered models, the best model is chosen based on:
Highest Adjusted R² (explains more variance).
Lowest Standard Error (reduces prediction uncertainty).
Selected Best Model
Variables Selected: 'Square_Feet', 'Num_Bedrooms', 'Num_Bathrooms', 'Num_Floors', 'Year_Built', 'Has_Garden', 'Has_Pool'
Cp = 8.00, which is within the accepted range.
Adjusted R² = 0.9438, indicating that the model explains 94.38% of the variance in house prices.
Standard Error = 10,161.7181, reflecting the average deviation of predicted prices from actual values.

Model Summary (OLS Results)
High R² (0.945) confirms that the model is a strong predictor of house prices.
F-statistic (1196, p-value < 0.0001) suggests a significant overall model fit.
Durbin-Watson (2.074) indicates no serious autocorrelation in residuals.
Condition Number (1.1e+05) suggests potential multicollinearity, though VIF checks confirmed it was within acceptable limits.
Interpretation of Coefficients
Square_Feet: For every additional square foot, price increases by $987.89.
Num_Bedrooms: Each additional bedroom increases price by $49,740.
Num_Bathrooms: Each additional bathroom adds $31,430.
Num_Floors: More floors contribute $19,700 to price.
Year_Built: Newer houses add $1,516 per year.
Has_Garden: Houses with gardens cost $28,800 more on average.
Has_Pool: Properties with pools are valued $42,650 higher.
"""

# Residual Analysis
residuals = best_model[4].resid
fitted_values = best_model[4].fittedvalues

plt.figure(figsize=(8, 5))
sns.scatterplot(x=y, y=residuals, alpha=0.6)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel("Price")
plt.ylabel("Residuals")
plt.title("Residuals vs. Price")
plt.show()

sm.qqplot(residuals, line='45', fit=True)
plt.title("Q-Q Plot of Residuals")
plt.show()

# Goldfeld-Quandt Test for Homoscedasticity
# Import the required function
from statsmodels.stats.diagnostic import het_goldfeldquandt

gq_test = het_goldfeldquandt(residuals, X, split=0.5)
print("Goldfeld-Quandt Test:")
print(f"F-statistic: {gq_test[0]:.4f}, p-value: {gq_test[1]:.4f}")
if gq_test[1] > 0.05:
    print("Conclusion: Fail to reject null hypothesis. Variances are equal (Homoscedasticity holds).")
else:
    print("Conclusion: Reject null hypothesis. Variances are not equal (Heteroscedasticity present).")

"""Based on the plot of residuals vs Price, the linearity assumption holds as there is no specific pattern in the plot.
Based on the durbin-watson previously calculated,it indicates no serious autocorrelation in residuals.
Base on the normality plot, the normality assumption holds.
Based on the GQ test ,variances are equal.

"""